# -*- coding: utf-8 -*-
"""Análisis_Exploratorio_De_Datos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOFKVE0-4tHCdXvLV14hDTlTSZawnycu

#**Análisis Exploratorio de Datos**

## ¿Qué es el Análisis Exploratorio de Datos (AED)?

El Análisis Exploratorio de Datos (AED) es una fase crucial en el proceso de análisis de datos. Consiste en la exploración y visualización inicial de un conjunto de datos para obtener una comprensión básica de su estructura, características y posibles patrones. A través del AED, los analistas de datos pueden descubrir relaciones interesantes, detectar valores atípicos y formular hipótesis iniciales antes de aplicar técnicas de modelado más avanzadas.

### Importancia del Análisis Exploratorio de Datos

El AED juega un papel fundamental en el proceso de análisis de datos por varias razones:

1. **Comprensión Inicial de los Datos:** Permite a los analistas familiarizarse con los datos antes de embarcarse en tareas de modelado más complejas. Esto ayuda a evitar malentendidos y errores que podrían surgir si no se comprenden completamente los datos.

2. **Identificación de Datos Faltantes y Valores Atípicos:** Durante el AED, es posible detectar datos faltantes o anomalías que podrían requerir atención especial en el preprocesamiento de datos.

3. **Selección de Técnicas de Modelado Adecuadas:** El AED ayuda a determinar qué técnicas de modelado son las más apropiadas para el conjunto de datos en cuestión. Al comprender la distribución y características de los datos, es más fácil elegir el enfoque de modelado adecuado.

4. **Formulación de Hipótesis Iniciales:** A través de gráficos y estadísticas básicas, el AED permite formular hipótesis iniciales sobre las relaciones entre las variables, lo que guiará el análisis más detallado posteriormente.

5. **Comunicación de Resultados:** El AED puede proporcionar información visual que facilita la comunicación de resultados y hallazgos a otras partes interesadas en el proyecto.

## Identificación y Manejo de Valores Atípicos

<img src="https://1.bp.blogspot.com/-R9IuLT4sxvs/XxCmbSZqwfI/AAAAAAAAgbE/NouH6CKIwjwbGj98gB2iGHcmv9VotcWAwCLcBGAsYHQ/s1600/ejemplo%2Bde%2Boutiler.png">

### Introducción a los Valores Atípicos

Los valores atípicos (outliers) son observaciones que se desvían significativamente del resto de los datos en un conjunto. Estos valores pueden tener un gran impacto en el análisis estadístico y los modelos predictivos, distorsionando las estimaciones y resultados. Por lo tanto, es fundamental identificar y manejar adecuadamente los valores atípicos durante el análisis exploratorio de datos.

### Método de los Cuartiles

<img src="https://i0.wp.com/lasmatesfaciles.com/wp-content/uploads/2021/06/image-1.png?resize=555%2C228&ssl=1">

El Método de los Cuartiles es una técnica estadística comúnmente utilizada para identificar valores atípicos en un conjunto de datos. Para ello, se calculan los cuartiles (Q1, Q2, Q3) y el rango intercuartil (RIC). Luego, cualquier valor que esté por debajo de Q1 - 1.5 * RIC o por encima de Q3 + 1.5 * RIC se considera un valor atípico.

##**Descripción de las Variables**:
El dataset contiene 30 características reales, calculadas a partir de una imagen digitalizada de una muestra de tumor, describiendo las características de los núcleos celulares presentes en la imagen. Las características están agrupadas en 10 categorías, y para cada categoría, se calculan la media, el error estándar y el peor (o mayor valor).

<u>**Categorías y Variables**</u>

1.**Radio (radius)**
 - **mean radius**: Promedio del radio de las células del tumor.
 - **radius error**: Error estándar del radio.
 - **worst radius**: Valor más alto del radio.


2.**Textura (texture)**
- **mean texture**: Promedio de la desviación estándar de los valores de los píxeles en la imagen.
- **texture error**: Error estándar de la textura.
- **worst texture**: Valor más alto de la textura.


3.**Perímetro (perimeter)**
- **mean perimeter**: Promedio del perímetro del tumor.
- **perimeter error**: Error estándar del perímetro.
- **worst perimeter**: Valor más alto del perímetro.


4.**Área (area)**
- **mean area**: Promedio del área del tumor.
- **area error**: Error estándar del área.
- **worst area**: Valor más alto del área.

5.**Suavidad (smoothness)**

- **mean smoothness**: Promedio de la varianza local en la longitud de los radios.
- **smoothness error**: Error estándar de la suavidad.
- **worst smoothness**: Valor más alto de la suavidad.


6.**Compacidad (compactness)**

- **mean compactness**: Promedio de la fórmula: (perímetro^2 / área - 1.0).
- **compactness error**: Error estándar de la compacidad.
- **worst compactness**: Valor más alto de la compacidad.


7.**Concavidad (concavity)**

- **mean concavity**: Promedio de la severidad de las porciones cóncavas del contorno.
- **concavity error**: Error estándar de la concavidad.
- **worst concavity**: Valor más alto de la concavidad.


8.**Puntos cóncavos (concave points)**

- **mean concave points**: Promedio del número de porciones cóncavas del contorno.
- **concave points error**: Error estándar de los puntos cóncavos.
- **worst concave points**: Valor más alto de los puntos cóncavos.

9.**Simetría (symmetry)**

- **mean symmetry**: Promedio de la simetría.
- **symmetry error**: Error estándar de la simetría.
- **worst symmetry**: Valor más alto de la simetría.

10.**Dimensión fractal (fractal dimension)**

- **mean fractal dimension**: Promedio de la "aproximación del contorno fractal" (1 - dimensión fractal).
- **fractal dimension error**: Error estándar de la dimensión fractal.
- **worst fractal dimension**: Valor más alto de la dimensión fractal.

**<u>Descripción de las Clases</u>**:

El dataset también contiene un vector objetivo (target) que indica la clase de cada muestra:

- 0: Maligno
- 1: Benigno

Estas variables proporcionan una descripción detallada de los núcleos celulares en imágenes de tumores, y pueden ser usadas para entrenar modelos de aprendizaje automático para clasificar tumores como benignos o malignos.

##**Cargar el dataset y ver sus características**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer

# Cargamos el conjunto de datos de cáncer de mama
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)

# Agregar la columna objetivo
df['target'] = data.target

df

# Identificación de valores atípicos utilizando el método de los cuartiles
# Vamos a utilizar la columna 'mean radius' para este ejemplo
column_name = 'mean radius'

Q1 = df[column_name].quantile(0.25)
Q3 = df[column_name].quantile(0.75)
RIC = Q3 - Q1
print(f"Q1: {Q1}\nQ3: {Q3}")
#Un 25% de los datos son un valor de 11.7 o menos
#Un 75% de los datos son un valor de 15.78 o menos
print(f"Rango intercuartil: {Q3-Q1}")
print(f"Distancia Outlier: {1.5*RIC}")

#Outliers
lower_bound = Q1 - 1.5 * RIC
upper_bound = Q3 + 1.5 * RIC
print(f"lower_bound: {lower_bound}")
print(f"upper_bound: {upper_bound}")

outliers = df[(df[column_name] < lower_bound) | (df[column_name] > upper_bound)]

outliers

#Extreme outliers

extreme_lower_bound = Q1-3*RIC
extreme_upper_bound = Q3+3*RIC

print(f"extreme_lower_bound: {extreme_lower_bound}")
print(f"extreme_upper_bound: {extreme_upper_bound}")

outliersext = df[(df[column_name] < extreme_lower_bound) | (df[column_name] > extreme_upper_bound)]
outliersext

# Visualización de valores atípicos utilizando un diagrama de caja y bigotes (boxplot)
plt.figure(figsize=(8, 6))
plt.boxplot(df[column_name], vert=False)
plt.scatter(outliers[column_name], [1]*len(outliers), color='red', label='Valores Atípicos')
plt.title("Diagrama de Caja y Bigotes - Valores Atípicos")
plt.xlabel(column_name)
plt.legend()
plt.show()

"""### Componentes del Diagrama de Caja y Bigotes:

- **Caja (Box):** La caja representa el rango intercuartil (IQR), que es el rango donde se encuentra el 50% medio de los datos. Los límites inferior y superior de la caja son el primer cuartil (Q1, el 25% inferior de los datos) y el tercer cuartil (Q3, el 75% superior de los datos), respectivamente.
- **Línea en la caja (Mediana):** La línea dentro de la caja indica la mediana de los datos, que es el valor central que divide los datos en dos mitades iguales.
- **Bigotes (Whiskers):** Los bigotes se extienden desde los cuartiles hasta los valores más bajos y más altos dentro de 1.5 veces el rango intercuartil (IQR). Los bigotes muestran la extensión de los datos dentro de este rango.
- **Puntos fuera de los bigotes (Valores atípicos):** Los puntos individuales que se encuentran fuera de los bigotes son considerados valores atípicos. Estos son valores que se encuentran a más de 1.5 veces el IQR desde los cuartiles.

### Interpretación del Gráfico:

- **Mediana:** La mediana del "mean radius" está representada por la línea naranja dentro de la caja. Esto indica el valor central del radio medio de las células del tumor.
- **Caja (IQR):** La caja muestra que la mayor parte de los valores del "mean radius" se encuentran entre aproximadamente 12 y 16 unidades. Esto es el rango intercuartil, que contiene el 50% medio de los datos.
- **Bigotes:** Los bigotes se extienden desde los límites de la caja hasta valores más extremos, aproximadamente entre 10 y 18 unidades. Esto indica que la mayoría de los datos se encuentran dentro de este rango.
- **Valores Atípicos:** Los puntos rojos a la derecha del gráfico representan valores atípicos. Estos son valores de "mean radius" que se encuentran significativamente por encima del rango típico, más allá de 18 unidades. Estos valores atípicos pueden indicar muestras con radios inusualmente grandes.

### Conclusiones:

- La mayor parte de los valores del "mean radius" están concentrados entre 12 y 16 unidades.
- Existen varios valores atípicos por encima de 18 unidades, indicando que hay algunas muestras con radios significativamente más grandes que la mayoría.
- La presencia de estos valores atípicos podría influir en los análisis posteriores y puede ser importante investigarlos más a fondo para entender su impacto.

Este diagrama es útil para identificar rápidamente la distribución de los datos, la presencia de valores atípicos y la dispersión de los valores de "mean radius" en el dataset.








"""

# Visualización de la distribución de 'mean radius' utilizando un histograma
plt.figure(figsize=(8, 6))
plt.hist(df[column_name], bins=20, edgecolor='black')
plt.scatter(outliers[column_name], np.zeros(len(outliers)), color='red', label='Valores Atípicos')
plt.title("Distribución de " + column_name)
plt.xlabel(column_name)
plt.ylabel("Frecuencia")
plt.legend()
plt.show()

# Información sobre los valores atípicos encontrados
print("Información sobre los valores atípicos:")
print(outliers)

"""Los valores atípicos o outliers pueden afectar la interpretación de los resultados. En el histograma, los valores atípicos pueden aparecer como barras solitarias o como barras muy altas o bajas en comparación con el resto.

### **Componentes del Histograma**:

- **Eje X (mean radius):** Representa los valores del radio medio de las células del tumor.
- **Eje Y (Frecuencia):** Representa el número de muestras que caen dentro de cada intervalo de valores de "mean radius".
- **Barras Azules:** Indican la frecuencia de las muestras en cada intervalo de valores del "mean radius".
- **Puntos Rojos:** Representan los valores atípicos de "mean radius".

### **Interpretación**:

#### Distribución General:

- La mayoría de los valores de "mean radius" se concentran entre aproximadamente 10 y 15 unidades.
- El histograma muestra una distribución sesgada a la derecha, con un mayor número de muestras con valores de radio más pequeños y una disminución gradual en la frecuencia a medida que los valores de radio aumentan.

#### Frecuencias Más Altas:

- Las frecuencias más altas se observan en el rango de 10 a 12 unidades, con el pico alrededor de 11 unidades.
- Esto sugiere que la mayoría de las células tumorales tienen un radio medio en este rango.

#### Valores Atípicos:

- Los puntos rojos a la derecha del histograma indican valores atípicos. Estos valores están significativamente por encima del rango típico, superando las 20 unidades.
- Estos valores atípicos pueden indicar muestras con radios inusualmente grandes y pueden necesitar una revisión adicional para entender su naturaleza.

#### Variabilidad:

- Hay una variabilidad considerable en los valores de "mean radius", con algunas muestras alcanzando hasta 25 unidades.
- La presencia de valores atípicos y la amplia dispersión de los valores indican que hay una diversidad considerable en los tamaños de los radios de las células tumorales.

### Conclusiones:

- La mayor parte de los valores del "mean radius" están concentrados entre 10 y 15 unidades, con un pico alrededor de 11 unidades.
- Existen varios valores atípicos por encima de 20 unidades, indicando que algunas muestras tienen radios significativamente más grandes.
- La distribución sesgada a la derecha sugiere que aunque la mayoría de las muestras tienen radios más pequeños, hay una minoría significativa con radios más grandes.

Este histograma es útil para entender la distribución general de los valores de "mean radius" en el dataset y para identificar la presencia de valores atípicos que podrían influir en los análisis posteriores.

### Manejo de Valores Atípicos
Una vez identificados los valores atípicos, existen diversas estrategias para manejarlos, dependiendo del contexto y el propósito del análisis. Algunas opciones comunes son:

1. **Remoción de Valores Atípicos:** Si los valores atípicos son el resultado de errores o mediciones incorrectas, se pueden eliminar del conjunto de datos. Sin embargo, esta estrategia debe realizarse con cuidado, ya que la eliminación de datos puede afectar la representatividad del conjunto.
2. **Transformación de Datos:** Aplicar transformaciones matemáticas como la transformación logarítmica o la transformación de raíz cuadrada puede reducir el impacto de los valores atípicos.
3. **Imputación de Valores:** Si los valores atípicos son el resultado de mediciones válidas, se pueden imputar con valores razonables basados en otros datos o técnicas de interpolación.
Es importante recordar que el manejo de valores atípicos debe basarse en un análisis cuidadoso y en el conocimiento del dominio del problema.

## Identificación y Eliminación de Datos Duplicados

### Introducción a los Datos Duplicados

Los datos duplicados son registros repetidos que aparecen más de una vez en un conjunto de datos. Estos datos redundantes pueden afectar negativamente el análisis y conducir a conclusiones incorrectas. Por lo tanto, es crucial identificar y eliminar los datos duplicados durante el análisis exploratorio de datos.

### Identificación de Datos Duplicados

Para identificar datos duplicados en un conjunto de datos, debemos comparar cada registro con todos los demás registros y encontrar aquellos que son idénticos. En Pandas, podemos utilizar la función `duplicated()` para encontrar filas duplicadas y `drop_duplicates()` para eliminarlas.

A continuación, continuaremos utilizando el conjunto de datos de viviendas cargado anteriormente y realizaremos la identificación y eliminación de datos duplicados:
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer

# Cargamos el conjunto de datos de cáncer de mama
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Agregar datos duplicados
df = pd.concat([df, df.sample(10)])

# Identificación de datos duplicados
duplicated_rows = df[df.duplicated()]
num_duplicates = len(duplicated_rows)

print("df.duplicated()")
print(df.duplicated())
print("\nnum_duplicates_rows")
print(duplicated_rows)
print("\nnum_duplicates")
print(num_duplicates)

# Eliminación de datos duplicados
df_cleaned = df.drop_duplicates()

print(f"Se encontraron {num_duplicates} filas duplicadas en el conjunto de datos.")

"""### Validación de la Eliminación de Datos Duplicados
Es importante verificar que los datos duplicados se hayan eliminado correctamente. Podemos volver a ejecutar el código para encontrar duplicados en el nuevo DataFrame df_cleaned y asegurarnos de que no haya más registros duplicados presentes.
"""

duplicated_rows_cleaned = df_cleaned[df_cleaned.duplicated()]
num_duplicates_cleaned = len(duplicated_rows_cleaned)

print(f"Después de la eliminación, se encontraron {num_duplicates_cleaned} filas duplicadas en el conjunto de datos limpio.")

"""# **Ejemplo 1 - Dataset Cáncer**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.decomposition import PCA

# Cargamos el conjunto de datos de cáncer de mama
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Información general del conjunto de datos
print("Información general del conjunto de datos:")
print(df.info())

# Descripción estadística
print("\nDescripción estadística del conjunto de datos:")
print(df.describe())

# Valores faltantes
print("\nValores faltantes en el conjunto de datos:")
print(df.isnull().sum())

# Visualización de la variable objetivo (target)
plt.figure(figsize=(6, 4))
sns.countplot(x='target', data=df)
plt.title("Distribución del diagnóstico")
plt.xlabel("Diagnóstico")
plt.ylabel("Frecuencia")
plt.xticks([0, 1], ['Benigno', 'Maligno'])
plt.show()

"""### Interpretación del Histograma: Distribución del diagnóstico de la variable target o dependiente

#### Componentes del Histograma:

- **Eje X (Diagnóstico):** Representa las clases de diagnóstico de las muestras del tumor (Benigno y Maligno).
- **Eje Y (Frecuencia):** Representa el número de muestras para cada clase de diagnóstico.
- **Barras Azules:** Indican la frecuencia de las muestras para cada clase de diagnóstico (Benigno y Maligno).

#### Interpretación:

**Distribución General:**

- La distribución muestra que hay más muestras clasificadas como malignas que benignas.
- El número de muestras malignas es significativamente mayor, con más de 350 muestras en comparación con alrededor de 200 muestras benignas.

**Frecuencias:**

- **Maligno:** Aproximadamente 350 muestras son clasificadas como malignas.
- **Benigno:** Aproximadamente 200 muestras son clasificadas como benignas.

**Conclusiones:**

- El dataset tiene una mayor proporción de muestras malignas en comparación con las benignas.
- La diferencia en la cantidad de muestras entre las dos clases puede influir en los análisis y modelos predictivos, ya que los modelos pueden estar sesgados hacia la clase mayoritaria (maligna).
- Es importante tener en cuenta esta distribución al realizar análisis y al construir modelos predictivos para asegurarse de que se manejen adecuadamente las posibles desbalanceaciones en las clases.

Este histograma es útil para entender la distribución general de las clases de diagnóstico en el dataset y para identificar posibles desbalanceaciones que podrían afectar los análisis posteriores.

###Formulación de hipótesis para una prueba estadística

Hipótesis para comparar las medias de dos grupos independientes (tumores malignos y tumores benignos) utilizando una prueba de hipótesis para la diferencia de medias.

- Hipótesis nula ($H_0$): No hay diferencia significativa en el radio promedio entre tumores malignos y tumores benignos.

- Hipótesis alternativa ($H_1$): Existe una diferencia significativa en el radio promedio entre tumores malignos y tumores benignos.
"""

from scipy.stats import ttest_ind

# Pruebas estadísticas para analizar diferencias entre grupos
# Comparación de 'mean radius' entre tumores malignos (target=0) y benignos (target=1)
malignant_radius = df[df['target'] == 0]['mean radius']
benign_radius = df[df['target'] == 1]['mean radius']

# Prueba t de Student para muestras independientes
t_stat, p_value = ttest_ind(malignant_radius, benign_radius, equal_var=False)

print("Resultados de la prueba t de Student:")
print(f"T-estadística: {t_stat}")
print(f"Valor p: {p_value}")

alpha = 0.05
if p_value < alpha:
    print("Hay evidencia suficiente para rechazar la hipótesis nula.")
else:
    print("No hay evidencia suficiente para rechazar la hipótesis nula.")

# Matriz de correlación
correlation_matrix = df.corr()

# Filtrar las correlaciones mayores en valor absoluto de 0.5
strong_correlations = correlation_matrix[np.abs(correlation_matrix) > 0.5]
plt.figure(figsize=(10, 8))
sns.heatmap(strong_correlations, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Correlaciones mayores en valor absoluto de 0.5")
plt.show()

# Filtrar las correlaciones menores o iguales a 0.5
weak_correlations = correlation_matrix[np.abs(correlation_matrix) <= 0.5]
plt.figure(figsize=(10, 8))
sns.heatmap(weak_correlations, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Correlaciones menores o iguales a 0.5")
plt.show()

# Identificación de valores atípicos en la columna 'mean radius'
Q1 = df['mean radius'].quantile(0.25)
Q3 = df['mean radius'].quantile(0.75)
RIC = Q3 - Q1
lower_bound = Q1 - 1.5 * RIC
upper_bound = Q3 + 1.5 * RIC
outliers = df[(df['mean radius'] < lower_bound) | (df['mean radius'] > upper_bound)]

outliers

# Identificación de valores atípicos
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.iloc[:,25 :-1])
plt.title("Diagrama de Caja y Bigotes - Valores Atípicos")
plt.xticks(rotation=45)
plt.show()

# Análisis por diagnóstico
diagnosis_groups = df.groupby('target')
for col in data.feature_names[:5]:
    plt.figure(figsize=(6, 4))
    for name, group in diagnosis_groups:
        sns.kdeplot(group[col], label=name)
    plt.title(f"Distribución de {col} por diagnóstico")
    plt.xlabel(col)
    plt.legend()
    plt.show()

"""El objetivo de este código es realizar un análisis comparativo de la distribución de las primeras cinco características ('mean radius', 'mean texture', 'mean perimeter', 'mean area' y 'mean smoothness') del conjunto de datos breast_cancer según el diagnóstico, que es la variable objetivo ('target'). El diagnóstico tiene dos posibles valores: 0 para tumores malignos y 1 para tumores benignos.

Permite identificar visualmente cómo difieren las distribuciones de estas características entre los dos grupos de diagnóstico.

En este tipo de gráficos de densidad comparativos para diferentes características según el diagnóstico (tumores malignos y benignos), se pueden observar varias diferencias esenciales que pueden proporcionar información valiosa sobre cómo se distribuyen las características en cada grupo. Algunas de las diferencias importantes a observar son:

1. **Superposición o separación de densidades:** Se debe prestar atención a si las curvas de densidad para el diagnóstico maligno y benigno se superponen o están claramente separadas. Una mayor separación indica que la característica puede ser más relevante para la clasificación, ya que tiene valores más distintos entre los grupos.
1. **Moda y forma de las densidades:** Identificar si las densidades tienen una o múltiples modas (picos). Una característica con múltiples modas puede indicar subgrupos dentro de los tumores malignos o benignos.
1. **Áreas de mayor o menor densidad:** Observar en qué rangos de valores la densidad es más alta o más baja para cada diagnóstico. Pueden existir diferencias significativas en la concentración de valores en ciertas áreas de las características.
1. **Escalas de los ejes:** Verificar si los ejes tienen la misma escala para todas las características o si varían significativamente. Esto puede afectar la interpretación de las diferencias entre las densidades.
1. **Presencia de colas:** Observar si las distribuciones tienen colas largas o asimetrías, ya que esto puede indicar la presencia de valores atípicos o información importante en los extremos de la distribución.
1. **Cantidad de solapamiento:** Si hay solapamiento entre las curvas de densidad, es posible que la característica no sea tan relevante para diferenciar entre tumores malignos y benignos. En cambio, si no hay solapamiento o es mínimo, la característica puede ser más discriminante.
1. **Tendencias generales:** Identificar patrones o tendencias generales en las diferencias de densidad entre los diagnósticos que puedan ser útiles para el análisis y la interpretación.
"""

# Análisis de correlación con el diagnóstico (con la variable objetivo)
correlation_with_target = df.corr()['target'].sort_values(ascending=False)
print("\nCorrelación con el diagnóstico:")
print(correlation_with_target)

"""# **2. Ejemplo 2 - Dataset Diabetes (ejercicio)**"""

#Para herramientas matemáticas de Machine Learning
import numpy as np
#Libreria de carga, manejo y manipulación de los datos
import pandas as pd
#Sublibrerias para Representación gráfica, enfocada en dibujos y gráficos
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from scipy.stats import ttest_ind
#libreria para normalización
from sklearn.preprocessing import StandardScaler

#Biblioteca de visualización de datos en Python
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


# Cargamos el conjunto de datos de diabetes
data = load_diabetes(scaled=False)
df = pd.DataFrame(data.data, columns=data.feature_names)
df['progression'] = data.target
y_progression = data.target  # Variable progresión original
df['progression'] = y_progression

df.columns

#Veemos el dataframe
X = df.iloc[:,:-1]
y = df.iloc[:,10]

df

# Exploración inicial de datos
print("Información general del conjunto de datos:")
print(df.info())
print("\nPrimeros registros del conjunto de datos:")
print(df.head())

# Estadísticas descriptivas del conjunto de datos
print("\nEstadísticas descriptivas del conjunto de datos:")
print(df.describe())

#Valores faltantes
print("\nValores faltantes en el conjunto de datos:")
print(df.isnull().sum())

# Identificación de datos duplicados
duplicated_rows = df[df.duplicated()]
num_duplicates = len(duplicated_rows)

print("df.duplicated()")
print(df.duplicated())
print("\nnum_duplicates_rows")
print(duplicated_rows)
print("\nnum_duplicates")
print(num_duplicates)

# Eliminación de datos duplicados
df_cleaned = df.drop_duplicates()

print(f"Se encontraron {num_duplicates} filas duplicadas en el conjunto de datos.")

# Distribución de la variable 'progression'

target_variable = 'progression'

plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='progression', bins=20, kde=True)
plt.title(f"Distribución de {target_variable}")
plt.xlabel(f"Nivel de {target_variable}")
plt.ylabel("Frecuencia")
plt.show()

"""###Conclusión
El gráfico muestra la distribución de la variable "progression", que representa el progreso de la diabetes en un paciente después de un año. La distribución es bimodal, con dos picos principales: uno alrededor de 50 y otro alrededor de 250. Esto indica que hay dos grupos principales de pacientes: aquellos con un progreso relativamente bajo (alrededor de 50) y aquellos con un progreso relativamente alto (alrededor de 250).

También se observa que la distribución está ligeramente sesgada hacia la derecha, lo que significa que hay más pacientes con un progreso relativamente alto que con un progreso relativamente bajo.

Es importante tener en cuenta que estas son solo conclusiones generales que se pueden extraer del gráfico. Para obtener un análisis más detallado de la distribución de la variable "progression", se necesitaría realizar un análisis estadístico más profundo.
"""

# Identificación de valores atípicos
plt.figure(figsize=(12, 6))
sns.boxplot(data=df.iloc[:,:-1])
plt.title("Diagrama de Caja y Bigotes - Valores Atípicos")
plt.xticks(rotation=45)
plt.show()

# Matriz de correlación
correlation_matrix = df.corr()

# Filtrar las correlaciones mayores en valor absoluto de 0.5
strong_correlations = correlation_matrix[np.abs(correlation_matrix) > 0.5]
plt.figure(figsize=(10, 8))
sns.heatmap(strong_correlations, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Correlaciones mayores en valor absoluto de 0.5")
plt.show()

# Filtrar las correlaciones menores o iguales a 0.5
weak_correlations = correlation_matrix[np.abs(correlation_matrix) <= 0.5]
plt.figure(figsize=(10, 8))
sns.heatmap(weak_correlations, cmap='coolwarm', annot=True, fmt=".2f", linewidths=0.5)
plt.title("Correlaciones menores o iguales a 0.5")
plt.show()

#GRÁFICOS DE CORRELACIONES DE LAS VARIABLES CON RESPECTO A LA VARIABLE OBJETIVO

# Obtener el nombre de la variable objetivo
target_variable = 'progression'

# Obtener los nombres de las variables x
x_variables = [col for col in df.columns if col != target_variable]

# Crear gráficos de dispersión automáticos
for x_var in x_variables:
    sns.regplot(x=x_var, y=target_variable, data=df)
    plt.title(f'Gráfico de {x_var} vs {target_variable}')
    plt.xlabel(x_var)
    plt.ylabel(target_variable)
    plt.show()

"""###Conclusión Matriz de correlaciones

La matriz de correlaciones muestra las relaciones lineales entre las variables del dataset load_diabetes. Las correlaciones se representan mediante números que van desde -1 hasta 1. Un valor de -1 indica una correlación negativa perfecta, lo que significa que las dos variables están inversamente relacionadas. Un valor de 1 indica una correlación positiva perfecta, lo que significa que las dos variables están directamente relacionadas. Un valor de 0 indica que no hay correlación entre las dos variables.

En la matriz de correlaciones, podemos observar que:
-  la variable "progression" tiene una correlación positiva moderada con las variables "age", "bmi", "bp", "s1", "s2", "s3", "s4" y "s5". Esto significa que estas variables están asociadas con un mayor progreso de la diabetes.
- La variable "sex" tiene una correlación negativa moderada con la variable "progression", lo que significa que las mujeres tienen un menor progreso de la diabetes que los hombres.

Es importante tener en cuenta que las correlaciones no implican causalidad. El hecho de que dos variables estén correlacionadas no significa que una cause la otra. Es posible que haya otros factores que expliquen la relación entre las variables.
"""

# Análisis de correlación con el diagnóstico (con la variable objetivo)
correlation_with_target = df.corr()['progression'].sort_values(ascending=False)
print("\nCorrelación con el diagnóstico:")
print(correlation_with_target)

import pandas as pd
from scipy.stats import ttest_ind

print("PRUEBA ESTADÍSTICA P-VALUE AGE VS PROGRESSION")
# Dividir los datos en dos grupos (X y Y)
group_X = df['age']
group_Y = df['progression']

# Realizar la prueba t de Student
t_statistic, p_value = ttest_ind(group_X, group_Y)

# Definir nivel de significancia
alpha = 0.05

# Imprimir resultados de la prueba t de Student
print("\nT-Statistic:", t_statistic)
print("P-Value:", p_value)

# Realizar pruebas de hipótesis
if p_value < alpha:
    print("Rechazar la hipótesis nula: Las medias de los grupos son diferentes.")
    print("\nPor lo tanto, la variable se mantiene y NO SE ELIMINA del modelo")
else:
    print("No se puede rechazar la hipótesis nula: No hay suficiente evidencia para decir que las medias son diferentes.")
    print("\nPor lo tanto, la variable no se mantiene y SE ELIMINA del modelo")

import pandas as pd
from scipy.stats import ttest_ind

# Obtener el nombre de la variable objetivo del data frame
target_variable = 'progression'

# Obtener los nombres de las variables x
x_variables = [col for col in df.columns if col != target_variable]

# Realizar pruebas de p-value automáticas
for x_var in x_variables:
    x_values = df[x_var]
    y_values = df[target_variable]

    t_statistic, p_value = ttest_ind(x_values, y_values)

    print(f'Variable X: {x_var}')
    print(f'T-Statistic: {t_statistic}')
    print(f'P-Value: {p_value}')

    alpha = 0.05
    if p_value < alpha:
        print("Rechazar la hipótesis nula: Las medias son diferentes.")
        print("Por lo tanto, la variable se mantiene y NO SE ELIMINA del modelo")
    else:
        print("No se puede rechazar la hipótesis nula: No hay suficiente evidencia para decir que las medias son diferentes.")
        print("Por lo tanto, la variable no se mantiene y SE ELIMINA del modelo")

    print('-' * 40)

"""## Transformar a problema de clasificación"""

import numpy as np
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Cargar el dataset load_diabetes
data = load_diabetes(scaled=False)

df = pd.DataFrame(data.data, columns=data.feature_names)
y_progression = data.target  # Variable progresión original

# Definir el umbral para la clasificación
umbral = 150  # Ejemplo de umbral, puedes ajustarlo según sea necesario
y_binary = np.where(y_progression >= umbral, 1, 0)

df['crítica_level'] = y_binary

df

import matplotlib.pyplot as plt
import seaborn as sns

# Análisis de la variable dependiente "crítica_level"

# Configurar el estilo de seaborn
sns.set(style="whitegrid")

# Crear el gráfico de barras
plt.figure(figsize=(8, 5))
ax = sns.countplot(x="crítica_level", data=df)


# Etiquetar las barras con las frecuencias
for p in ax.patches:
    ax.annotate(format(p.get_height(), '.0f'),
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points')

# Título y ejes del gráfico
plt.title("Distribución de 'crítica_level'")
plt.xlabel("Nivel Crítico")
plt.ylabel("Número de Casos")
plt.show()

"""###Conclusión

El gráfico muestra la distribución de la variable target "critical_level" del dataset load_diabetes. La variable "critical_level" es una variable categórica que tiene tres niveles: 0, 1 y 2. El nivel 0 indica que el paciente no tiene un nivel crítico de diabetes, el nivel 1 indica que el paciente tiene un nivel moderado de diabetes y el nivel 2 indica que el paciente tiene un nivel alto de diabetes.

El gráfico muestra que la mayoría de los pacientes (aproximadamente el 60%) tienen un nivel crítico de diabetes de 0. Esto significa que la mayoría de los pacientes no tienen un nivel crítico de diabetes. Sin embargo, hay un número significativo de pacientes (aproximadamente el 40%) que tienen un nivel crítico de diabetes de 1 o 2. Esto significa que hay un número significativo de pacientes que tienen un nivel moderado o alto de diabetes.

Es importante tener en cuenta que este gráfico solo muestra la distribución de la variable "critical_level". No proporciona ninguna información sobre las relaciones entre la variable "critical_level" y las otras variables del dataset. Para obtener más información sobre estas relaciones, se debe realizar un análisis estadístico más profundo.texto en cursiva
"""

# Matriz de correlación
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", linewidths=.5)
plt.title("Matriz de Correlación")
plt.show()

"""###Conclusión
La matriz de correlaciones muestra la fuerza y dirección de la relación lineal entre las variables del dataset load_diabetes. Las correlaciones se representan mediante valores numéricos entre -1 y 1. Una correlación de 0 indica que no hay relación lineal entre las variables. Una correlación positiva indica que las variables se mueven en la misma dirección, mientras que una correlación negativa indica que las variables se mueven en direcciones opuestas.

En la matriz de correlaciones, se observan las siguientes relaciones:

- La variable "critical_level" tiene una correlación positiva moderada con la variable "age" (0.34). Esto significa que a medida que la edad del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.
- La variable "critical_level" tiene una correlación positiva moderada con la variable "bmi" (0.26). Esto significa que a medida que el índice de masa corporal del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.
- La variable "critical_level" tiene una correlación positiva débil con la variable "bp" (0.24). Esto significa que a medida que la presión arterial del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.
- La variable "critical_level" tiene una correlación positiva débil con la variable "s1" (0.22). Esto significa que a medida que la concentración de suero de tipo s1 del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.
- La variable "critical_level" tiene una correlación positiva débil con la variable "s2" (0.2). Esto significa que a medida que la concentración de suero de tipo s2 del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.
- La variable "critical_level" tiene una correlación positiva débil con la variable "s3" (0.19). Esto significa que a medida que la concentración de suero de tipo s3 del paciente aumenta, también aumenta la probabilidad de que tenga un nivel crítico de diabetes más alto.

Es importante tener en cuenta que la correlación no implica causalidad. El hecho de que dos variables estén correlacionadas no significa que una cause la otra. Es posible que haya otros factores que causen que ambas variables aumenten o disminuyan. Se debe realizar un análisis estadístico más profundo para determinar si existe una relación causal entre las variables.
"""

import pandas as pd
from scipy.stats import ttest_ind

# Obtener el nombre de la variable objetivo del data frame
target_variable = 'crítica_level'

# Obtener los nombres de las variables x
x_variables = [col for col in df.columns if col != target_variable]

# Realizar pruebas de p-value automáticas
for x_var in x_variables:
    x_values = df[x_var]
    y_values = df[target_variable]

    t_statistic, p_value = ttest_ind(x_values, y_values)

    print(f'Variable X: {x_var}')
    print(f'T-Statistic: {t_statistic}')
    print(f'P-Value: {p_value}')

    alpha = 0.05
    if p_value < alpha:
        print("Rechazar la hipótesis nula: Las medias son diferentes.")
        print("Por lo tanto, la variable se mantiene y NO SE ELIMINA del modelo")
    else:
        print("No se puede rechazar la hipótesis nula: No hay suficiente evidencia para decir que las medias son diferentes.")
        print("Por lo tanto, la variable no se mantiene y SE ELIMINA del modelo")

    print('-' * 40)

import statsmodels.api as sm

# Pruebas de hipótesis (usando statsmodels)
X = df.drop("crítica_level", axis=1)
X = sm.add_constant(X)  # Agregar constante para la regresión
y = df["crítica_level"]

model = sm.Logit(y, X)
result = model.fit()

print("="*90)

"""$$
\ln\left(\frac{p}{1 - p}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k
$$

Donde:
- ($\ln$) es la función logaritmo natural,
- ($p$) es la probabilidad de éxito (en nuestro caso, la probabilidad de que "crítica\_level" sea 1)
- ($\beta_0$, $\beta_1$, $\beta_2$, $\ldots$, $\beta_k$\) son los coeficientes de regresión para las variables independientes ($x_1$, $x_2$, $\ldots$, $x_k$).

<br>

\begin{align*}
\text{Hipótesis Nula ($H_0$):} & \quad \beta_i = 0 \quad \text{(La variable no tiene efecto)} \\
\text{Hipótesis Alternativa ($H_1$):} & \quad \beta_i \neq 0 \quad \text{(La variable tiene efecto)}
\end{align*}
"""

import statsmodels.api as sm

# Pruebas de hipótesis (usando statsmodels)
X = df.drop("crítica_level", axis=1)
X = sm.add_constant(X)  # Agregar constante para la regresión
y = df["crítica_level"]

model = sm.Logit(y, X)
result = model.fit()

print("="*90)

# Crear una tabla para mostrar los resultados y la relevancia de variables
table_data = result.summary().tables[1].data[1:]  # Excluir la primera fila (encabezados)
columns = ["X","Coeficiente", "Std Err", "Z", "P>|z|", "[0.025", "0.975]"]
summary_df = pd.DataFrame(table_data, columns=columns)

# Convertir las columnas numéricas a tipo float
for column in columns:
  if(column != "X"):
    summary_df[column] = pd.to_numeric(summary_df[column], errors="coerce")
  else:
    pass

# Agregar la columna "Variable Relevante" basada en el p-value y una significancia de 0.05
summary_df["Relevante"] = summary_df["P>|z|"].apply(lambda p: "Sí" if p <= 0.05 else "No")

# Imprimir la tabla con la decisión de relevancia
print(summary_df)

"""#**Conclusion general**

Acerca del análisis exploratorio puedo comentar que:
1. Dependiendo del tipo de la variable predictora *$y$*, específicamente, teniendo un caso de un problema de **regresión** y otro de **clasificación**, se pueden apreciar que existen ciertas variables que no son significativos o no ayudan a explicar la variabilidad de la variable respuesta para el caso del problema de clasificación.
En cambio, para el problema de regresión se tiene que todas las variables son significativas para ayudar a explicar la variabilidad de la variable respuesta.

  - Para el caso del **problema de clasificación**, se tienen que las variables que no ayudan a explicar la variabilidad de la variable respuesta son las siguientes:
    - age
    - s1
    - s2
    - s3
    - s4
    - s6
  
  Por lo que, por decisión del analista pudiesen ser eliminadas o trabajadas con otras variables con el fin de determianr su relevancia en el modelo o no.
__(Cabe destacar que lo anterior es teniendo en cuenta un α=0.05)__



---


###Recordar
**Factor de Inflación de la Varianza (VIF)**: cuantifica la intensidad de la multicolinealidad en un análisis de regresión normal de mínimos cuadrados.

$$
VIF = \frac{1}{1 - R^2}
$$

Recordar que La experiencia indica que si cualquiera de los  VIF es mayor que 10, es indicio de que los coeficientes asociados de regresión están mal estimados debido a la multicolinealidad.
(En algunos casos el VIF no debe de ser mayor a 5)

Por lo tanto, podemos despejar R^2 para determinar el valor máximo que se puede tener para no encontrarnos con el problema de multicolinealidad.

- Para el caso de un **VIF que no sea mayor a 10**
  $$
  10 >= \frac{1}{1 - R^2}
  $$

  Luego,
  $$
  10(1-R^2) <= 1
  $$

  $$
  10-10R^2 <= 1
  $$

  $$
  9 <= 10R^2
  $$

  $$
  \frac{9}{10} <= R^2
  $$


  $$
  \sqrt{0.9}<= R
  $$
Finalmente,
  $$
  \pm 0.9486<= R
  $$
Por lo tanto, la correlación no puede exceder del valor de 0.9486 para que no exista multicolinealidad entre las variables predichas.


- Para el caso de un **VIF que no sea mayor a 5**
  $$
  5 >= \frac{1}{1 - R^2}
  $$

  Luego,
  $$
  5(1-R^2) <= 1
  $$

  $$
  5-5R^2 <= 1
  $$

  $$
  4 <= 5R^2
  $$

  $$
  \frac{4}{5} <= R^2
  $$


  $$
  \sqrt{0.8}<= R
  $$
Finalmente,
  $$
  \pm 0.8944<= R
  $$
Por lo tanto, la correlación no puede exceder del valor de 0.8944 para que no exista multicolinealidad entre las variables predichas.


---


2. Al observar la matriz de correlación se pueden observar problemas de multicolinealidad entre las siguientes variables predichas ($x$):
 - Considerando cualquier criterio de VIF que no sea mayor a 5 o a 10, se aprecia que las variables s1 y s2 existe multicolinealidad al presentar una correlación de 0.9

3. Podemos inferir de los gráficos de correlación de las variables independientes con respecto a la variable dependiente que:
  - Entre la variable dependiente progression y cada una de las variables independientes age, sex, bmi, bp, s1, s2, s4, s5, s6 existe dependencia lineal positiva puesto que la tendencia es creciente en cada caso.
  - Por otro lado, existe una dependencia lineal negativa entre la variable s3 y la variable dependiente progression.
  - También destacar que no existe o es casi nula la dependencia lineal entre la variable sex y la variable dependdiente progression
"""